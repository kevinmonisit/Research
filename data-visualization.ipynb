{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Absenteeism in Toms River High School East**\n![banner.jpg](https://www.trschools.com/hseast/imgs/banner.jpg)\n\n***\n\n## Introduction\nToms River High School East (TRSHE), a comprehensive four-year high school located in New Jersey, is affected by large absent rates. In fact, the chronic absenteeism rates in TRSHE has been above the NJ state average rates from grades 9th to 12th. This kernel's objective is to better understand the underlying patterns of TRSHE absenteeism, how certain factors contribute to these patterns, and the type of methods that are best for prediction.\n\nT-Test to check for significance.\nData binning\n\nCheck 3D for Tardies and Absent relationships\n\nOutline:\n1. [Data Overview](#section-one)","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nfrom IPython.display import Image\nimport os\n\nstudent_data = pd.read_csv('../input/dataproj/High School East Student Data - Sheet1.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-01T18:11:21.111223Z","iopub.execute_input":"2021-11-01T18:11:21.111846Z","iopub.status.idle":"2021-11-01T18:11:22.147878Z","shell.execute_reply.started":"2021-11-01T18:11:21.111804Z","shell.execute_reply":"2021-11-01T18:11:22.146928Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# 1. Data Overview\n\nThe data was collected through the help of Mrs. Anders and Dr. Kretz, both faculty of TRSHE. Certain variables were chosen because chronic absenteeism is susceptible to variables like **Limited English Proficiency, 504/IDEA Disability, Race/Ethnicity**, and likewise. A comprehensive and large dataset of students in the United States conducted by the National Center for Education Statistics exists, but many important variables were surpressed for public-use. Accessing that data is a **rigorous** and **security-tight** process involving various academic officers that no regular person could pass. So that wasn't happening. So I resorted to my school data.\n\nThus, the data collected for this project was **manually** recorded. \n***\n\n**A6-A12**: represents absences from 6th grade to 12th grade\n\n**T6-T12**: represents tardies from 6th grade to 12th grade\n\n**IEP/Specialized**: represents whether a student is in special education","metadata":{}},{"cell_type":"code","source":"student_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:22.149787Z","iopub.execute_input":"2021-11-01T18:11:22.150117Z","iopub.status.idle":"2021-11-01T18:11:22.189251Z","shell.execute_reply.started":"2021-11-01T18:11:22.150086Z","shell.execute_reply":"2021-11-01T18:11:22.188018Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(student_data.iloc[0].values[5:12]) # absence columns\nprint(student_data.iloc[0].values[12:19]) #tardy columns","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:22.191119Z","iopub.execute_input":"2021-11-01T18:11:22.191579Z","iopub.status.idle":"2021-11-01T18:11:22.199800Z","shell.execute_reply.started":"2021-11-01T18:11:22.191532Z","shell.execute_reply":"2021-11-01T18:11:22.198830Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Because some absent and tardy count columns contains a string (\"TRANSFER\"), some of the variables in the **A6** to **A12** and **T6** to **T12** columns are represented as strings instead of integers. So I made some adjustments and converted the necessary values to int values. \n\nhttps://stackoverflow.com/questions/59084770/one-hot-encoder-what-is-the-industry-norm-to-encode-before-train-split-or-after\n\n## 1.1 Preparing Data for Graph\n\nI make a separate instance of student_data because I will be preproccessing data before the train test split in order for searborn and matplotlib to work. Here, I convert number values that are strings into an int type even if the string indicates a float type (some students have absences like 2.5 for a given year). Then, I convert \"TRANSFER\" to equal 0.","metadata":{}},{"cell_type":"code","source":"dataForGraph = pd.read_csv('../input/dataproj/High School East Student Data - Sheet1.csv')\n\n#easy way of accessing A_6, A_7, ... A_N columns\ndef column_list(letter, start, end):\n    return [\"%s%d\" % (letter, i) for i in range(start, end)]\n\n#convert strings to int type even if it's a float\ndef convertStat(x):\n    \n    if(isinstance(x, int) == False):\n        \n        #we don't know if the string is float or int\n        #converting it to int if it's not an int will cause an error\n        try:\n            return 0 if x == \"TRANSFER\" else int(x)\n        except:\n            pass\n        \n        #if it can't pass as an int, then it must be a float that will be converted to an int\n        #the float is rounded\n        return 0 if x == \"TRANSFER\" else int(float(x))\n    else:\n        return x\n\n#convert absent and tardy columsn to integers\nfor i in [\"A\", \"T\"]:\n    for j in column_list(i, 6, 13):\n        dataForGraph[j] = dataForGraph[j].apply(convertStat)\n\nprint(list(dataForGraph.iloc[0].values[5:12])) # absence columns of first student\nprint(list(dataForGraph.iloc[0].values[12:19])) #tardy columns\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:22.201238Z","iopub.execute_input":"2021-11-01T18:11:22.201561Z","iopub.status.idle":"2021-11-01T18:11:22.240293Z","shell.execute_reply.started":"2021-11-01T18:11:22.201502Z","shell.execute_reply":"2021-11-01T18:11:22.239313Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\ndataForGraph['AbsentSum'] = dataForGraph[column_list('A', 6, 13)].sum(axis=1)\ndataForGraph['TardySum'] = dataForGraph[column_list('T', 6, 13)].sum(axis=1)\n\n\n#for different time periods\ndataForGraph['AbsencesSum_MS'] = dataForGraph[column_list('A', 6, 9)].sum(axis=1)\ndataForGraph['AbsencesSum_HS'] = dataForGraph[column_list('A', 9, 13)].sum(axis=1)\n\ndataForGraph['TardiesSum_MS'] = dataForGraph[column_list('T', 6, 9)].sum(axis=1)\ndataForGraph['TardiesSum_HS'] = dataForGraph[column_list('T', 9, 13)].sum(axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:22.243289Z","iopub.execute_input":"2021-11-01T18:11:22.243624Z","iopub.status.idle":"2021-11-01T18:11:22.270417Z","shell.execute_reply.started":"2021-11-01T18:11:22.243593Z","shell.execute_reply":"2021-11-01T18:11:22.269423Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Graphing the Data","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,7))\n#plt.figure(figsize=(2,10))\n\nsns.set_style(\"ticks\")\n\n# 6:13 represents the absent columns\n# 13:20 represents the tardy columns\nx_values = range(6,13)\nfor i in range(len(student_data)):\n    absences_y = dataForGraph.iloc[i].values[5:12]\n    tardies_y = dataForGraph.iloc[i].values[12:19]\n\n    ax1.plot(x_values, np.array(absences_y), alpha=0.7)    \n    ax2.plot(x_values, np.array(tardies_y), alpha=0.7)\n\nax1.set_title(\"Absences from 6th to 12th grade\")\nax2.set_title(\"Tardies from 6th to 12th grade\")\n\nax1.set_ylabel(\"Absences\")\nax2.set_ylabel(\"Tardies\")\n\nax2.set_xlabel(\"Grade\")\nax1.set_xlabel(\"Grade\")\n\nplt.subplots_adjust(wspace=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:22.272634Z","iopub.execute_input":"2021-11-01T18:11:22.273396Z","iopub.status.idle":"2021-11-01T18:11:22.898750Z","shell.execute_reply.started":"2021-11-01T18:11:22.273346Z","shell.execute_reply":"2021-11-01T18:11:22.897347Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\n# middle school tardies vs middle school absences\n\ndef create_graph(ax, x, y, xlabel, ylabel, title):\n    ax.title.set_position([.5, 1.05])\n    sns.scatterplot(x=x, y=y, hue=\"Student\", data=dataForGraph, legend=False, ax=ax)\n    ax.set(xlabel=xlabel, ylabel=ylabel, title=title)\n\n    \nwith sns.axes_style(\"white\"):\n    fg, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2,2, figsize=(15,7))\n    sns.despine(fig=fg)\n    create_graph(ax1, \"TardySum\", \"AbsentSum\", \"Sum of Tardies\", \"Sum of Absences\", \"Relationship between Tardies and Absences\")\n\n    create_graph(ax2, \"AbsencesSum_MS\", \n                 \"AbsencesSum_HS\", \n                 \"Middle School Absences\", \n                 \"High School Absences\", \n                 \"Relationship between High School and Middle School Absences\")\n\n    create_graph(ax3, \"TardiesSum_MS\", \n                 \"TardiesSum_HS\", \n                 \"Tardies in Middle School\", \n                 \"Tardies in High School\", \n                 \"Relationship between High School and Middle School Tardies\")\n\n    create_graph(ax4, \"TardiesSum_MS\", \n                 \"AbsencesSum_HS\", \n                 \"Tardies in Middle School\", \n                 \"Absences in High School\", \n                 \"Relationship between Tardies and Absences in Middle/High School\")\n\n\nplt.subplots_adjust(wspace=1, top=2)\n\nprint(\"Corrleration between Absences in MS and HS: \")\nprint(np.corrcoef(dataForGraph[\"AbsencesSum_MS\"], dataForGraph[\"AbsencesSum_HS\"]))\n\nprint(\"Corrleration between Tardies in MS and Absences HS:% i\")\nprint(np.corrcoef(dataForGraph[\"TardiesSum_MS\"], dataForGraph[\"AbsencesSum_HS\"]))\n\n\nprint(\"Corrleration between Tardies in MS and Tardies HS:% i\")\nprint(np.corrcoef(dataForGraph[\"TardiesSum_MS\"], dataForGraph[\"TardiesSum_HS\"]))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:22.900946Z","iopub.execute_input":"2021-11-01T18:11:22.901718Z","iopub.status.idle":"2021-11-01T18:11:23.599168Z","shell.execute_reply.started":"2021-11-01T18:11:22.901665Z","shell.execute_reply":"2021-11-01T18:11:23.597891Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"In the above scatterplots, tardies in middle school or in general do not seem to have a strong linear relationship with the number of absences. ","metadata":{}},{"cell_type":"code","source":"plt.clf()\n\n#sns.distplot(student_data[\"AbsencesSum_HS\"], bins=5, kde=False)\nHistbins = range(0,100,10)\n\n#plt.hist(dataForGraph[\"AbsencesSum_HS\"], bins=Histbins, edgecolor=\"black\")\n\ndef ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n    # Number of data points: n\n    n = len(data)\n\n    # x-data for the ECDF: x\n    x = np.sort(data)\n\n    # y-data for the ECDF: y\n    y = np.arange(1, n+1) / n\n\n    return x, y\n\nstudents = dataForGraph[\"AbsencesSum_HS\"]\n\nx_, y_ = ecdf(students)\n\nstudents = dataForGraph[\"TardiesSum_HS\"]\nh, g = ecdf(students)\n\nwith sns.axes_style(\"white\"):\n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(17,6))\n    sns.despine(fig=fig)\n    \n    sns.distplot(dataForGraph[\"AbsencesSum_HS\"], bins=Histbins, kde=False, rug=False, ax=ax1, hist_kws=dict(alpha=0.5))\n    sns.distplot(dataForGraph[\"TardiesSum_HS\"], bins=Histbins, kde=False, rug=False, ax=ax1, hist_kws=dict(alpha=0.3))\n    ax1.set_xlabel(\"Total Absences in Middle and High School\")\n    ax1.set_ylabel(\"Number of Students\")\n    \n    #sns.boxplot(x=\"AbsencesSum_HS\", data=dataForGraph, ax=ax2)\n    #sns.swarmplot(y=\"AbsencesSum_HS\", data=dataForGraph)\n    sns.scatterplot(x=x_, y=y_)\n    sns.scatterplot(x=h, y=g)\n    \n    ax2.set_xlabel(\"Total Absences in High School\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:23.600701Z","iopub.execute_input":"2021-11-01T18:11:23.601059Z","iopub.status.idle":"2021-11-01T18:11:24.297827Z","shell.execute_reply.started":"2021-11-01T18:11:23.601025Z","shell.execute_reply":"2021-11-01T18:11:24.296776Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"\n[Based on TRSHE data,](https://rc.doe.state.nj.us/report.aspx?type=school&lang=english&county=29&district=5190&school=030&schoolyear=2018-2019#P99cba7ec593f446e8cbf8d62c3db0208_2_oHit0) the chronic absent rates grow throughout the grade levels. At grade 12, **21**% of students, 2% higher than the NJ state average, have been chronically absent. In grade 11, **20**% of TRSHE students were chronically absent, which is 6% higher than the NJ state average.\n\nFor our dataset, **what is the distrubtion of tardies/absences over the grades, and what are some variable relationships?**","metadata":{}},{"cell_type":"code","source":"plt.clf()\n\nnumOfStudents, cols = dataForGraph.shape\n\nabsentColumns = column_list(\"A\", 9, 13)\nbarplot_absences = dataForGraph[absentColumns]\n\npercentageOfChronic = \\\n    100 * np.array([len(barplot_absences[barplot_absences[i] >= 18]) for i in absentColumns]) / numOfStudents\n\nwith sns.axes_style(\"whitegrid\"):    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,7))\n    sns.despine(fig=fig)\n    sns.barplot(data=barplot_absences, ax=ax1, ci=None)\n    ax1.title.set_position([.5, 1.05])\n    ax1.set_xticklabels([\"9th\", \"10th\", \"11th\", \"12th\"])\n    ax1.set_title(\"Average Absences During High School for 2020 Seniors\")\n    ax1.set_ylabel(\"Average Absences\")\n\n    plt.ylim([0,100])\n    sns.barplot(x=list(range(0, len(percentageOfChronic))), y=percentageOfChronic.tolist(), ax=ax2)\n    ax2.title.set_position([.5, 1.05])\n    ax2.set_xticklabels([\"9th\", \"10th\", \"11th\", \"12th\"])\n    ax2.set_title(\"Percentage of Chronically Absent 2020 Seniors during High School\")\n    ax2.set_ylabel(\"Percentage of Students\")\n    \n#perhaps look for 2020 student data and compare the two findings???\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:24.299776Z","iopub.execute_input":"2021-11-01T18:11:24.300154Z","iopub.status.idle":"2021-11-01T18:11:24.586797Z","shell.execute_reply.started":"2021-11-01T18:11:24.300116Z","shell.execute_reply":"2021-11-01T18:11:24.585633Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(\"Percentage of Chronic Absentees by HS Grade in Sample\")\nfor i in list(zip(column_list(\"Absences in \", 9, 13), percentageOfChronic.tolist())):\n    print(i)\n    \nImage('../input/testimage/Reserved.ReportViewerWebControl-1.png')","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:24.588476Z","iopub.execute_input":"2021-11-01T18:11:24.588854Z","iopub.status.idle":"2021-11-01T18:11:24.609551Z","shell.execute_reply.started":"2021-11-01T18:11:24.588817Z","shell.execute_reply":"2021-11-01T18:11:24.608597Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Absences are lower in the 12th grade because of **COVID-19**: absences were not counted in the standard fashion (missing a class or even the first class of the day would not count towards absences). What's good about these statistics is that the chronic absent rate across grade levels in high school closely mimics the NJ School Performance Report for 2018-2019, an official report that considers all students. That means that the dataset wasn't aquired in a manner that wouldn't fit real life (e.g. skewed). Therefore, it would be a good representation of real life despite the dataset being much smaller.\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Setting Up Learning Models\n\nNext, I will split the actual student_data dataframe and use prior proprocessing methods created in the creation of the graphs above. I will do minor proprocessing before the cross validations of each model.","metadata":{}},{"cell_type":"code","source":"random_seed = 1\ntest_size = 0.2\n\nfeatures = [\"A6\", \"A7\", \"A8\", \"A9\", \"Gender\", \"IEP/Specialized\"]\n\nfor i in [\"A\", \"T\"]:\n    for j in column_list(i, 6, 13):\n        student_data[j] = student_data[j].apply(convertStat)\n\n\nstudent_data['AbsencesSum_HS'] = student_data[column_list('A', 9, 13)].sum(axis=1)\nstudent_data['AbsencesSum_MS'] = student_data[column_list('A', 6, 9)].sum(axis=1)\n\nstudent_data[\"Gender\"] = pd.get_dummies(student_data[\"Gender\"])\nstudent_data[\"IEP/Specialized\"] = pd.get_dummies(student_data[\"IEP/Specialized\"])\n\ny = student_data[\"AbsencesSum_HS\"]\n\ny = y.between(y.quantile(0), y.quantile(.95))\ndropIndex = []\n\nprint(student_data.shape)\n\nfor i in range(0, len(student_data)):\n    if(~y[i] == True):\n        student_data.drop(i, inplace=True)\n      \nprint(student_data.shape)\ny = student_data[\"AbsencesSum_HS\"]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:24.611233Z","iopub.execute_input":"2021-11-01T18:11:24.611673Z","iopub.status.idle":"2021-11-01T18:11:24.659669Z","shell.execute_reply.started":"2021-11-01T18:11:24.611633Z","shell.execute_reply":"2021-11-01T18:11:24.658626Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\nstudent_data['TardiesSum_MS'] = student_data[column_list('T', 6, 9)].sum(axis=1)\n\ny = student_data[\"AbsencesSum_HS\"]\nx = student_data[\"AbsencesSum_MS\"]\n\n\nfig, ax = plt.subplots(1,1)\nax.title.set_position([.5, 1.05])\nsns.scatterplot(x=x, y=y, hue=\"Student\", data=student_data, legend=False, ax=ax)\n#ax.set(xlabel=xlabel, ylabel=ylabel, title=title)\n\nprint(np.corrcoef(x,y))","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:24.661023Z","iopub.execute_input":"2021-11-01T18:11:24.661331Z","iopub.status.idle":"2021-11-01T18:11:24.846635Z","shell.execute_reply.started":"2021-11-01T18:11:24.661300Z","shell.execute_reply":"2021-11-01T18:11:24.845500Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"After manipulating the dataset so that it can be processed by the learning models, I create methods to automate the process of comparing learning models.\n\nhttps://machinelearningmastery.com/train-final-machine-learning-model/\n\nhttps://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/\nhttps://github.com/justmarkham/scikit-learn-videos/blob/master/07_cross_validation.ipynb\n\nhttps://www.itl.nist.gov/div898/handbook/eda/section3/histogr6.htm - what to do to measure skewed distributions","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    \n    return mae\n\n\ndef runModels(models, trainX, trainY, cv):\n    cross_valid = []\n    for i in models:\n        model = i[1]\n        model.fit(trainX, trainY)\n        scores = -1 * cross_val_score(model, \n            trainX,\n            trainY,\n            cv=cv,\n            scoring=\"neg_mean_absolute_error\")\n\n        cross_valid.append((i[0], scores.mean(), scores))\n\n    return cross_valid\n\nmodels = []\n\n\nfeatures = [\"A6\", \"A7\", \"A8\"]\n\nmodels.append(('XGBRegressor', \n    XGBRegressor(random_state=random_seed,\n        n_estimators=2000,\n        learning_rate=0.0006)))\n\nmodels.append(('DecisionTreeRegressor', \n    DecisionTreeRegressor(random_state=random_seed)))\n\nmodelOutcomes = runModels(models, \n    student_data[features], \n    student_data[\"AbsencesSum_HS\"], \n    5)\n\nfor i in modelOutcomes:\n    print(i)\n\nprint(features)\nscores = -1 * cross_val_score(DecisionTreeRegressor(random_state=1),\n                              student_data[features],\n                              student_data[\"AbsencesSum_HS\"],\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(np.mean(scores))","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:24.848069Z","iopub.execute_input":"2021-11-01T18:11:24.848403Z","iopub.status.idle":"2021-11-01T18:11:28.993214Z","shell.execute_reply.started":"2021-11-01T18:11:24.848368Z","shell.execute_reply":"2021-11-01T18:11:28.992108Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\n#split and preproccess\ntrain_X, val_X, train_y, val_y = train_test_split(student_data[features], student_data[\"AbsencesSum_HS\"], random_state=random_seed, test_size=test_size)\n\ntrain_X = pd.get_dummies(train_X)\nval_X = pd.get_dummies(val_X)\n\n# get_dummies creates different columns,but each set needs to have equal # of features\nmissing_cols = set( train_X.columns ) - set( val_X.columns )\nfor c in missing_cols:\n    val_X[c] = 0\n\nval_X = val_X[train_X.columns]\n\n#right now its a little weird, each possible value is its own column but the data works surprisingly well????\nprint(train_X)\n\nprint(\"Now the training and testing sets have equal number of columns after encoding their categorical values\")\nprint(val_X.shape)\nprint(train_X.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:28.994900Z","iopub.execute_input":"2021-11-01T18:11:28.995634Z","iopub.status.idle":"2021-11-01T18:11:29.030385Z","shell.execute_reply.started":"2021-11-01T18:11:28.995587Z","shell.execute_reply":"2021-11-01T18:11:29.029379Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot([1,2,3], [1,2,3])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:29.032020Z","iopub.execute_input":"2021-11-01T18:11:29.032689Z","iopub.status.idle":"2021-11-01T18:11:29.385092Z","shell.execute_reply.started":"2021-11-01T18:11:29.032646Z","shell.execute_reply":"2021-11-01T18:11:29.384061Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# 3. Learning Models\n\n## 3.1 Decision Trees/Random Forests\n\nObjective is to find the optimal number of leaves for each decision tree. The next step is to compare each model to each other for the most optimal model to use.","metadata":{}},{"cell_type":"code","source":"\"\"\"\n\nmodels.append(('LogisticRegression', \n    LogisticRegression(random_state=random_seed, max_iter=10000)))\n\n\nmodels.append(('DecisionTreeRegressor', \n    DecisionTreeRegressor(random_state=random_seed)))\n\nmodels.append(('XGBRegressor', \n    XGBRegressor(random_state=random_seed,\n        n_estimators=2000,\n        learning_rate=0.0006)))\n\nmodels.append(('SVC',\n    SVC(random_state=random_seed)))\n\nmodels.append((\"KNeighborsRegressor\",\n    KNeighborsRegressor()))\n\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:29.386469Z","iopub.execute_input":"2021-11-01T18:11:29.386788Z","iopub.status.idle":"2021-11-01T18:11:29.392842Z","shell.execute_reply.started":"2021-11-01T18:11:29.386758Z","shell.execute_reply":"2021-11-01T18:11:29.391906Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n\n#==================== Validate the Decision Tree Model ===========================\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    \n    return mae\n\nleaf_node_range = range(2,30)\nscores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in leaf_node_range}\nlow = min(scores, key=scores.get)\nstudent_tree_model = DecisionTreeRegressor(random_state=1, max_leaf_nodes=min(scores, key=scores.get))\nstudent_tree_model.fit(train_X, train_y)\n\nabsences_predictions = student_tree_model.predict(val_X)\nmae = mean_absolute_error(absences_predictions, val_y)\n\nprint(\"Validation MAE: {:,.0f}\".format(mae))","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:29.394262Z","iopub.execute_input":"2021-11-01T18:11:29.394604Z","iopub.status.idle":"2021-11-01T18:11:29.531861Z","shell.execute_reply.started":"2021-11-01T18:11:29.394572Z","shell.execute_reply":"2021-11-01T18:11:29.531021Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"https://towardsdatascience.com/why-random-forests-outperform-decision-trees-1b0f175a0b5\nhttps://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n\nhttps://towardsdatascience.com/optimizing-hyperparameters-in-random-forest-classification-ec7741f9d3f6","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\nrandom_forest = RandomForestRegressor(random_state=random_seed,\n                                    max_features=\"log2\",\n                                    n_estimators=9000,\n                                    max_leaf_nodes=12,\n                                    min_samples_split=12,\n                                    min_samples_leaf=40)\n\nmae = []\nrandom_forest.fit(train_X, train_y)\nabsences_predictions = random_forest.predict(val_X)\nprint(mean_absolute_error(absences_predictions, val_y))\n\nscores = -1 * cross_val_score(random_forest, train_X, train_y,\n                              cv=5,\n                            scoring='neg_mean_absolute_error')\n\nprint(scores.mean())\nprint(list(scores))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:11:29.533092Z","iopub.execute_input":"2021-11-01T18:11:29.533584Z","iopub.status.idle":"2021-11-01T18:13:05.328134Z","shell.execute_reply.started":"2021-11-01T18:11:29.533548Z","shell.execute_reply":"2021-11-01T18:13:05.326921Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression()))\n\ndef runModels(models, trainX, trainY, valY, cv):\n    cross_valid = []\n    for i in models:\n        model = i[1]\n        scores = -1 * cross_val_score(model, \n            trainX, \n            trainY,\n            cv=cv,\n            scoring=\"mean_absolute_error\")\n\n        cross_valid.append((i[0], scores.mean(), scores))\n\n    return cross_valid\n\nlogReg = LogisticRegression(random_state=1, max_iter=10000)\nlogReg.fit(train_X, train_y)\nabsences_predictions = logReg.predict(val_X)\nprint(mean_absolute_error(absences_predictions, val_y))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-01T18:13:05.330076Z","iopub.execute_input":"2021-11-01T18:13:05.330549Z","iopub.status.idle":"2021-11-01T18:13:05.702381Z","shell.execute_reply.started":"2021-11-01T18:13:05.330478Z","shell.execute_reply":"2021-11-01T18:13:05.701357Z"},"trusted":true},"execution_count":19,"outputs":[]}]}