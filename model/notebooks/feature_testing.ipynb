{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import model.model_setup as ms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "student_data, pre_process = ms.create_student_data(\"../../data/High School East Student Data - Sheet1.csv\")\n",
    "\n",
    "print(\"pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Feature Engineering Tests\n",
    "I create a new copy of the student_data because the student_data will serve as a model for the many feature engineering\n",
    "tests in this notebook.\n",
    "## 2.1 Binning the Data\n",
    "https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b\n",
    "### 2.1.1 Fixed-Width"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import copy as cp\n",
    "\n",
    "graph_binning_results = False\n",
    "\n",
    "model_pipeline = Pipeline(steps=[('number_fix', ms.SumTransformer(transformation=\"fixed\", bins=13)),\n",
    "                             #    ('preprocess', pre_process),\n",
    "                                 ('model', DecisionTreeRegressor(random_state=1))\n",
    "                                 ])\n",
    "\n",
    "data_copy = cp.deepcopy(student_data)\n",
    "\n",
    "#comparing fixed width binning and dynamic width binning\n",
    "data_copy[\"HS_AB_FIXED\"] = np.array(np.floor(np.array(data_copy[\"AbsencesSum_HS\"])))\n",
    "\n",
    "scores_fixed = np.array(ms.run_test(data_copy, \"HS_AB_FIXED\", model_pipeline, features_=features))\n",
    "\n",
    "ms.print_scores(scores_fixed)\n",
    "scores = []\n",
    "\n",
    "#set to True if you want to graph the binning results\n",
    "if graph_binning_results:\n",
    "    for i in range(1,100):\n",
    "        pipe = Pipeline(steps=[('number_fix', ms.SumTransformer(bins=i)),\n",
    "                                #     ('preprocess', pre_process),\n",
    "                                     ('model', DecisionTreeRegressor(random_state=1))\n",
    "                                     ])\n",
    "\n",
    "        scores.append(np.mean(np.array(ms.run_test(data_copy, \"HS_AB_FIXED\", pipe, features_=features))))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(1,100), scores)\n",
    "\n",
    "    fig.savefig(\"test.png\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [17.33731809 17.19736842 14.80473373 12.24116424 12.71544715]\n",
      "Score mean:  14.859206326363434\n",
      "Score variances:  4.612170978352201\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2 Log Binning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [21.96923077 16.69230769 14.69230769 16.95384615 20.16666667]\n",
      "Score mean:  18.094871794871796\n",
      "Score variances:  6.829911900065748\n"
     ]
    }
   ],
   "source": [
    "data_copy_log = cp.deepcopy(student_data)\n",
    "\n",
    "log_binning_pipeline_test = Pipeline(steps=[('number_fix', ms.SumTransformer(transformation=\"log\")),\n",
    "                            #     ('preprocess', pre_process),\n",
    "                                 ('model', DecisionTreeRegressor(random_state=1))\n",
    "                                 ])\n",
    "\n",
    "\n",
    "scores_log = np.array(ms.run_test(data_copy_log, \"AbsencesSum_HS\", log_binning_pipeline_test, features_=features))\n",
    "\n",
    "print_scores(scores_log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Scaling the Data\n",
    "### 2.2.1 Standard Scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [14.1173029  14.12586942 14.7003947  15.98329941 15.24343408]\n",
      "Score mean:  14.834060102961967\n",
      "Score variances:  0.504295881871075\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "data_copy_scale = cp.deepcopy(student_data)\n",
    "\n",
    "# scales the columns by looking at the whole thing before cross valid\n",
    "#for j in ms.column_list(\"A\", 6, 9):\n",
    "#   data_copy_scale[j] = StandardScaler().fit_transform(np.array(data_copy_scale[j]).reshape(-1,1))\n",
    "\n",
    "scale_process = ColumnTransformer(remainder='drop',\n",
    "                                transformers=[('Scaling', StandardScaler(), [\"A6\", \"A7\", \"A8\"])])\n",
    "\n",
    "standard_scale_pipeline = Pipeline(steps=[('number_fix', ms.SumTransformer(bins=7)),\n",
    "                                 ('preprocess', scale_process),\n",
    "                                 ('model', LinearRegression())\n",
    "                                 ])\n",
    "\n",
    "scores_scale = np.array(ms.run_test(data_copy_scale, \"AbsencesSum_HS\", standard_scale_pipeline, features_=features))\n",
    "print_scores(scores_scale)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When adding the standard scaler into the pipeline, there is no difference with or without scaling the absence columns.\n",
    "Because Decision Trees do not really care if a continuous feature is normally distributed, I changed it to Linear Regression.\n",
    "Scaling the entire columns before the cross validation into five different parts actually led to decreased performance\n",
    "and increased variation. It is probably because using StandardScaler is unnecessary since the columns are already\n",
    "the same units. Perhaps scaling the columns blurred the association between the number of absences in each grade\n",
    "and the total absences in high school. I'm not sure.\n",
    "\n",
    "What I am sure about is that Linear Regression has great results and a great score variance!\n",
    "\n",
    "### 2.2.2 Log Transformation\n",
    "\n",
    "Log transformations are used when there is a specific column that has a much higher variance than the other columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "could not convert string to float: 'TRANSFER'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36m_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merrstate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minvalid\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"ignore\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36mf\u001B[0;34m(values, axis, skipna, **kwds)\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 125\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0malt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskipna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mskipna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36mnanvar\u001B[0;34m(values, axis, skipna, ddof, mask)\u001B[0m\n\u001B[1;32m    761\u001B[0m     \u001B[0;31m# See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 762\u001B[0;31m     \u001B[0mavg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_ensure_numeric\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mcount\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    763\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0maxis\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/numpy/core/_methods.py\u001B[0m in \u001B[0;36m_sum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m     37\u001B[0m          initial=_NoValue, where=True):\n\u001B[0;32m---> 38\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mumr_sum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeepdims\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minitial\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwhere\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: 'TRANSFER'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-1c8c88bba27d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcolumn_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"A\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m9\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_copy_scale\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;31m#data_copy_scale[\"A6\"] = np.log(data_copy_scale[\"A6\"])\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36mstat_func\u001B[0;34m(self, axis, skipna, level, ddof, numeric_only, **kwargs)\u001B[0m\n\u001B[1;32m  11234\u001B[0m                 \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskipna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mskipna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mddof\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mddof\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m  11235\u001B[0m             )\n\u001B[0;32m> 11236\u001B[0;31m         return self._reduce(\n\u001B[0m\u001B[1;32m  11237\u001B[0m             \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumeric_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnumeric_only\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskipna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mskipna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mddof\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mddof\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m  11238\u001B[0m         )\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36m_reduce\u001B[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001B[0m\n\u001B[1;32m   3889\u001B[0m                 )\n\u001B[1;32m   3890\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merrstate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"ignore\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3891\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdelegate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskipna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mskipna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3892\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3893\u001B[0m         \u001B[0;31m# TODO(EA) dispatch to Index\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36m_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m                 \u001B[0;31m# object arrays that contain strings\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mis_object_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 76\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     77\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: could not convert string to float: 'TRANSFER'"
     ]
    }
   ],
   "source": [
    "for j in ms.column_list(\"A\", 6, 9):\n",
    "    print(data_copy_scale[j].var())\n",
    "#data_copy_scale[\"A6\"] = np.log(data_copy_scale[\"A6\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The variations are pretty much the same (maybe). If I do a log transformation, then I'd need to do it for all\n",
    "the columns. Log transforming the absences don't do that much. It has been tested: see feature binning by log above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Imputing the Data\n",
    "Testing different imputing methods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "scaler = StandardScaler()\n",
    "\n",
    "imputed_data = cp.deepcopy(student_data)\n",
    "\n",
    "# scales the columns by looking at the whole thing before cross valid\n",
    "#for j in ms.column_list(\"A\", 6, 9):\n",
    "#   data_copy_scale[j] = StandardScaler().fit_transform(np.array(data_copy_scale[j]).reshape(-1,1))\n",
    "\n",
    "impute_pre = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('Impute', SimpleImputer(strategy='mean'), [\"A6\", \"A7\", \"A8\"])\n",
    "                                              ])\n",
    "\n",
    "impute_pipeline = Pipeline(steps=[('number_fix', ms.SumTransformer(bins=12,  new_value=None)),\n",
    "                                 ('preprocess', impute_pre),\n",
    "                                 ('model', LinearRegression())\n",
    "                                 ])\n",
    "\"\"\"\n",
    "Strategy=Mean\n",
    "Scores:  [11.26007034 12.92051632 13.45643293 16.3414301  12.37170819]\n",
    "Score mean:  13.270031576344786\n",
    "Score variances:  2.8874648678364245\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "scores_impute = np.array(ms.run_test(imputed_data, \"AbsencesSum_HS\", impute_pipeline, features_=features))\n",
    "print_scores(scores_impute)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "model_pipeline.fit(student_data[features], student_data[\"AbsencesSum_HS\"])\n",
    "\n",
    "importances = model_pipeline.named_steps['Decision Tree'].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(7):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "scores = -1 * cross_val_score(ms.run_test(tree, \"Decision Tree\"),\n",
    "                              student_data[features],\n",
    "                              student_data[\"AbsencesSum_HS\"],\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "#print(np.mean(scores))\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing the optimal number of features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[('number_fix', ms.SumTransformer()),\n",
    "                                 ('pre_process', pre_process),\n",
    "                                 ('Decision Tree', tree)\n",
    "                                 ])\n",
    "\n",
    "\n",
    "print(\"hell\")\n",
    "%matplotlib inline\n",
    "\n",
    "#RVEFC for determining optimal number of features??"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}