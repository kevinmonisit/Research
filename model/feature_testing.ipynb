{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, KBinsDiscretizer, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "def print_scores(score_array):\n",
    "    print(\"Scores: \", score_array)\n",
    "    print(\"Score mean: \", np.mean(score_array))\n",
    "    print(\"Score variances: \", np.var(score_array))\n",
    "\n",
    "# easy way of accessing A_6, A_7, ... A_N columns\n",
    "def column_list(letter, start, end):\n",
    "    return [\"%s%d\" % (letter, i) for i in range(start, end)]\n",
    "\n",
    "\n",
    "def remove_outliers(column, target, first, second):\n",
    "    non_outliers = target.between(target.quantile(first), target.quantile(second))\n",
    "    count = 0\n",
    "\n",
    "    for index in range(0, len(column)):\n",
    "        if ~non_outliers[index]:\n",
    "            count += 1\n",
    "            column.drop(index, inplace=True)\n",
    "\n",
    "    print(\"%i outliers were removed\" % count)\n",
    "\n",
    "# convert strings to int type even if it's a float\n",
    "# replace by median or mean?\n",
    "def convert_stat(x, new_value=0):\n",
    "    if not isinstance(x, int):\n",
    "\n",
    "        if not isinstance(x, float) and '.' not in x:\n",
    "            return new_value if x == \"TRANSFER\" else int(x)\n",
    "        else:\n",
    "            return new_value if x == \"TRANSFER\" else int(float(x))\n",
    "\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def transform_value(x, imputer=None, bins=1, new_value=0):\n",
    "    value = convert_stat(x, new_value=new_value)\n",
    "\n",
    "    if imputer is None:\n",
    "        return np.floor(value / float(bins))\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "class SumTransformer(BaseEstimator):\n",
    "\n",
    "    # set new_value to None if Pipeline contains SimpleImputer\n",
    "    # this is for absences and tardies since somet students are\n",
    "    # transfer students. The placeholder in the CSV is the string \"TRANSFER\"\n",
    "    def __init__(self, new_value=0, bins=1, transformation=\"fixed\", imputer=None):\n",
    "        self.new_value = new_value\n",
    "        self.transformation = transformation\n",
    "        self.bins = bins\n",
    "        # SimpleImputer object\n",
    "        self.imputer = imputer\n",
    "\n",
    "        if self.new_value is None and self.imputer is None:\n",
    "            raise ValueError(\"New value has been set to None but imputer argument is also None.\")\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # change to for i in [\"A\", \"T\"] to include tardies if need be\n",
    "        for i in [\"A\"]:\n",
    "\n",
    "            # corrects the values in the data frame that will be used in the training models\n",
    "            for j in column_list(i, 6, 9):\n",
    "                # if no function is provided, the stats will be converted regularly\n",
    "                # where it can be divided into fixed-width bins\n",
    "                # though unnecessary, this is to make testing new things easier\n",
    "                if self.transformation == \"fixed\":\n",
    "                    df[j] = df[j].apply(transform_value, args=(self.imputer, self.bins, self.new_value))\n",
    "\n",
    "                elif self.transformation == \"log\":\n",
    "                    df[j] = df[j].apply(lambda x: np.log((1 + convert_stat(x, new_value=self.new_value))))\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"Transformation argument was not correctly assigned.\")\n",
    "\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Hold up a minute.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def run_test(data, target, pipeline, features_=[\"A6\", \"A7\", \"A8\"]):\n",
    "\n",
    "    scores_ = -1 * cross_val_score(pipeline,\n",
    "                              data[features_],\n",
    "                              data[target],\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores_\n",
    "\n",
    "print(\"pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "After setting up the functions needed for pre-processing, I set up the data. There are necessary pre-processing\n",
    "*before* the actual pre-processing. Because some numerical columns contain string types, it is necessary\n",
    "to convert them. Also, outliers are determined before the split (or pipeline), and removed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 outliers were removed\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "student_data = pd.read_csv(\"../data/High School East Student Data - Sheet1.csv\")\n",
    "features = [\"A6\", \"A7\", \"A8\"]\n",
    "student_data[\"AbsencesSum_HS\"] = 0\n",
    "\n",
    "# Pipeline doesn't allow transformations on the target label\n",
    "# so I have to do transformations outside of Pipeline in order\n",
    "# to sum all absences in High School for each student.\n",
    "for j in column_list(\"A\", 9, 13):\n",
    "    student_data[j] = student_data[j].apply(convert_stat)\n",
    "\n",
    "\n",
    "student_data[\"AbsencesSum_HS\"] = student_data[column_list('A', 9, 13)].sum(axis=1)\n",
    "\n",
    "# because we've created the total absences in high school column\n",
    "# we are now able to eliminate outliers in the dataset.\n",
    "remove_outliers(student_data, student_data[\"AbsencesSum_HS\"], 0, 0.95)\n",
    "\n",
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('categories', OneHotEncoder(), [\"Gender\", \"IEP/Specialized\"])])\n",
    "\n",
    "model_pipeline = Pipeline(steps=[('number_fix', SumTransformer()),\n",
    "                                 ('model', DecisionTreeRegressor(random_state=1))\n",
    "                                 ])\n",
    "\n",
    "#######################################\n",
    "# Sorta like unit testing but in jupyter\n",
    "test = run_test(student_data, \"AbsencesSum_HS\", model_pipeline)\n",
    "prior_run = np.array([23.43076923, 16.23076923, 15.76923077, 17.93846154, 19.58333333])\n",
    "\n",
    "if not np.allclose(test, prior_run, atol=0.001):\n",
    "    raise Exception(\"Modification to pre-processing led to unintended results.\")\n",
    "#######################################\n",
    "\n",
    "print(\"pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Feature Engineering Tests\n",
    "I create a new copy of the student_data because the student_data will serve as a model for the many feature engineering\n",
    "tests in this notebook.\n",
    "## 2.1 Binning the Data\n",
    "https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b\n",
    "### 2.1.1 Fixed-Width"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import copy as cp\n",
    "\n",
    "model_pipeline = Pipeline(steps=[('number_fix', SumTransformer(transformation=\"fixed\", bins=13)),\n",
    "                             #    ('preprocess', pre_process),\n",
    "                                 ('model', DecisionTreeRegressor(random_state=1))\n",
    "                                 ])\n",
    "\n",
    "data_copy = cp.deepcopy(student_data)\n",
    "\n",
    "#comparing fixed width binning and dynamic width binning\n",
    "data_copy[\"HS_AB_FIXED\"] = np.array(np.floor(np.array(data_copy[\"AbsencesSum_HS\"])))\n",
    "\n",
    "scores_fixed = np.array(run_test(data_copy, \"HS_AB_FIXED\", model_pipeline, features_=features))\n",
    "\n",
    "print_scores(scores_fixed)\n",
    "scores = []\n",
    "\n",
    "#set to True if you want to graph the binning results\n",
    "if False:\n",
    "    for i in range(1,100):\n",
    "        pipe = Pipeline(steps=[('number_fix', SumTransformer(bins=i)),\n",
    "                                #     ('preprocess', pre_process),\n",
    "                                     ('model', DecisionTreeRegressor(random_state=1))\n",
    "                                     ])\n",
    "\n",
    "        scores.append(np.mean(np.array(run_test(data_copy, \"HS_AB_FIXED\", pipe, features_=features))))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(1,100), scores)\n",
    "\n",
    "    fig.savefig(\"test.png\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [17.33731809 17.19736842 14.80473373 12.24116424 12.71544715]\n",
      "Score mean:  14.859206326363434\n",
      "Score variances:  4.612170978352201\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2 Log Binning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [21.96923077 16.69230769 14.69230769 16.95384615 20.16666667]\n",
      "Score mean:  18.094871794871796\n",
      "Score variances:  6.829911900065748\n"
     ]
    }
   ],
   "source": [
    "data_copy_log = cp.deepcopy(student_data)\n",
    "\n",
    "log_binning_pipeline_test = Pipeline(steps=[('number_fix', SumTransformer(transformation=\"log\")),\n",
    "                            #     ('preprocess', pre_process),\n",
    "                                 ('model', DecisionTreeRegressor(random_state=1))\n",
    "                                 ])\n",
    "\n",
    "\n",
    "scores_log = np.array(run_test(data_copy_log, \"AbsencesSum_HS\", log_binning_pipeline_test, features_=features))\n",
    "\n",
    "print_scores(scores_log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Scaling the Data\n",
    "### 2.2.1 Standard Scaling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [14.1173029  14.12586942 14.7003947  15.98329941 15.24343408]\n",
      "Score mean:  14.834060102961967\n",
      "Score variances:  0.504295881871075\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "data_copy_scale = cp.deepcopy(student_data)\n",
    "\n",
    "# scales the columns by looking at the whole thing before cross valid\n",
    "#for j in column_list(\"A\", 6, 9):\n",
    "#   data_copy_scale[j] = StandardScaler().fit_transform(np.array(data_copy_scale[j]).reshape(-1,1))\n",
    "\n",
    "scale_process = ColumnTransformer(remainder='drop',\n",
    "                                transformers=[('Scaling', StandardScaler(), [\"A6\", \"A7\", \"A8\"])])\n",
    "\n",
    "standard_scale_pipeline = Pipeline(steps=[('number_fix', SumTransformer(bins=7)),\n",
    "                                 ('preprocess', scale_process),\n",
    "                                 ('model', LinearRegression())\n",
    "                                 ])\n",
    "\n",
    "scores_scale = np.array(run_test(data_copy_scale, \"AbsencesSum_HS\", standard_scale_pipeline, features_=features))\n",
    "print_scores(scores_scale)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When adding the standard scaler into the pipeline, there is no difference with or without scaling the absence columns.\n",
    "Because Decision Trees do not really care if a continuous feature is normally distributed, I changed it to Linear Regression.\n",
    "Scaling the entire columns before the cross validation into five different parts actually led to decreased performance\n",
    "and increased variation. It is probably because using StandardScaler is unnecessary since the columns are already\n",
    "the same units. Perhaps scaling the columns blurred the association between the number of absences in each grade\n",
    "and the total absences in high school. I'm not sure.\n",
    "\n",
    "What I am sure about is that Linear Regression has great results and a great score variance!\n",
    "\n",
    "### 2.2.2 Log Transformation\n",
    "\n",
    "Log transformations are used when there is a specific column that has a much higher variance than the other columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "could not convert string to float: 'TRANSFER'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36m_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merrstate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minvalid\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"ignore\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36mf\u001B[0;34m(values, axis, skipna, **kwds)\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 125\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0malt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskipna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mskipna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    126\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36mnanvar\u001B[0;34m(values, axis, skipna, ddof, mask)\u001B[0m\n\u001B[1;32m    761\u001B[0m     \u001B[0;31m# See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 762\u001B[0;31m     \u001B[0mavg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_ensure_numeric\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mcount\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    763\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0maxis\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/numpy/core/_methods.py\u001B[0m in \u001B[0;36m_sum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m     37\u001B[0m          initial=_NoValue, where=True):\n\u001B[0;32m---> 38\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mumr_sum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeepdims\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minitial\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwhere\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: 'TRANSFER'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-1c8c88bba27d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcolumn_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"A\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m6\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m9\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_copy_scale\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;31m#data_copy_scale[\"A6\"] = np.log(data_copy_scale[\"A6\"])\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36mstat_func\u001B[0;34m(self, axis, skipna, level, ddof, numeric_only, **kwargs)\u001B[0m\n\u001B[1;32m  11234\u001B[0m                 \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskipna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mskipna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mddof\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mddof\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m  11235\u001B[0m             )\n\u001B[0;32m> 11236\u001B[0;31m         return self._reduce(\n\u001B[0m\u001B[1;32m  11237\u001B[0m             \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumeric_only\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnumeric_only\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskipna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mskipna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mddof\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mddof\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m  11238\u001B[0m         )\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36m_reduce\u001B[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001B[0m\n\u001B[1;32m   3889\u001B[0m                 )\n\u001B[1;32m   3890\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merrstate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"ignore\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3891\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdelegate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mskipna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mskipna\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3892\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3893\u001B[0m         \u001B[0;31m# TODO(EA) dispatch to Index\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/Research/lib/python3.8/site-packages/pandas/core/nanops.py\u001B[0m in \u001B[0;36m_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m                 \u001B[0;31m# object arrays that contain strings\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mis_object_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 76\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     77\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: could not convert string to float: 'TRANSFER'"
     ]
    }
   ],
   "source": [
    "for j in column_list(\"A\", 6, 9):\n",
    "    print(data_copy_scale[j].var())\n",
    "#data_copy_scale[\"A6\"] = np.log(data_copy_scale[\"A6\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The variations are pretty much the same (maybe). If I do a log transformation, then I'd need to do it for all\n",
    "the columns. Log transforming the absences don't do that much. It has been tested: see feature binning by log above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Imputing the Data\n",
    "Testing different imputing methods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "scaler = StandardScaler()\n",
    "\n",
    "imputed_data = cp.deepcopy(student_data)\n",
    "\n",
    "# scales the columns by looking at the whole thing before cross valid\n",
    "#for j in column_list(\"A\", 6, 9):\n",
    "#   data_copy_scale[j] = StandardScaler().fit_transform(np.array(data_copy_scale[j]).reshape(-1,1))\n",
    "\n",
    "impute_pre = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('Impute', SimpleImputer(strategy='mean'), [\"A6\", \"A7\", \"A8\"])\n",
    "                                              ])\n",
    "\n",
    "impute_pipeline = Pipeline(steps=[('number_fix', SumTransformer(bins=12,  new_value=None)),\n",
    "                                 ('preprocess', impute_pre),\n",
    "                                 ('model', LinearRegression())\n",
    "                                 ])\n",
    "\"\"\"\n",
    "Strategy=Mean\n",
    "Scores:  [11.26007034 12.92051632 13.45643293 16.3414301  12.37170819]\n",
    "Score mean:  13.270031576344786\n",
    "Score variances:  2.8874648678364245\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "scores_impute = np.array(run_test(imputed_data, \"AbsencesSum_HS\", impute_pipeline, features_=features))\n",
    "print_scores(scores_impute)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "model_pipeline.fit(student_data[features], student_data[\"AbsencesSum_HS\"])\n",
    "\n",
    "importances = model_pipeline.named_steps['Decision Tree'].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(7):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "scores = -1 * cross_val_score(run_test(tree, \"Decision Tree\"),\n",
    "                              student_data[features],\n",
    "                              student_data[\"AbsencesSum_HS\"],\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "#print(np.mean(scores))\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing the optimal number of features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(steps=[('number_fix', SumTransformer()),\n",
    "                                 ('pre_process', pre_process),\n",
    "                                 ('Decision Tree', tree)\n",
    "                                 ])\n",
    "\n",
    "\n",
    "print(\"hell\")\n",
    "%matplotlib inline\n",
    "\n",
    "#RVEFC for determining optimal number of features??"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}